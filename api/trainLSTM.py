from pyvi import ViTokenizer, ViPosTagger
import re 
from gensim.models import Word2Vec
import string
import codecs
import pandas as pd
import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM, Bidirectional
from keras.models import load_model
import random

class Train:
  def __init__(self):
    self.maxlen_sentence = 100
    self.epochs = 10
    self.batch_size = 20
    self.num_class = 2
    self.dropout = 0.2
    self.modelw2v = None
    self.modelLSTM = None
    self.randText = None
    self.label = None


  def no_marks(self,s):
    VN_CHARS_LOWER = u'áº¡áº£Ã£Ã Ã¡Ã¢áº­áº§áº¥áº©áº«Äƒáº¯áº±áº·áº³áºµÃ³Ã²á»Ãµá»Ã´á»™á»•á»—á»“á»‘Æ¡á»á»›á»£á»Ÿá»¡Ã©Ã¨áº»áº¹áº½Ãªáº¿á»á»‡á»ƒá»…ÃºÃ¹á»¥á»§Å©Æ°á»±á»¯á»­á»«á»©Ã­Ã¬á»‹á»‰Ä©Ã½á»³á»·á»µá»¹Ä‘Ã°'
    VN_CHARS_UPPER = u'áº áº¢ÃƒÃ€ÃÃ‚áº¬áº¦áº¤áº¨áºªÄ‚áº®áº°áº¶áº²áº´Ã“Ã’á»ŒÃ•á»Ã”á»˜á»”á»–á»’á»Æ á»œá»šá»¢á»á» Ã‰Ãˆáººáº¸áº¼ÃŠáº¾á»€á»†á»‚á»„ÃšÃ™á»¤á»¦Å¨Æ¯á»°á»®á»¬á»ªá»¨ÃÃŒá»Šá»ˆÄ¨Ãá»²á»¶á»´á»¸ÃÄ'
    VN_CHARS = VN_CHARS_LOWER + VN_CHARS_UPPER
    __INTAB = [ch for ch in VN_CHARS]
    __OUTTAB = "a"*17 + "o"*17 + "e"*11 + "u"*11 + "i"*5 + "y"*5 + "d"*2
    __OUTTAB += "A"*17 + "O"*17 + "E"*11 + "U"*11 + "I"*5 + "Y"*5 + "D"*2
    __r = re.compile("|".join(__INTAB))
    __replaces_dict = dict(zip(__INTAB, __OUTTAB))
    result = __r.sub(lambda m: __replaces_dict[m.group(0)], s)
    return result
  def normalize_text(self,text):
    path_nag = './data/nag1.txt'
    path_pos = './data/pos1.txt'
    path_not = './data/not.txt'
    with codecs.open(path_nag, 'r', encoding='UTF-8') as f:
        nag = f.readlines()
    nag_list1 = [' '+n.replace('\n', ' ') for n in nag]
    nag_list = [n.replace('\r','') for n in nag_list1]
    with codecs.open(path_pos, 'r', encoding='UTF-8') as f:
        pos = f.readlines()
    pos_list1 = [' '+n.replace('\n', ' ') for n in pos]
    pos_list = [n.replace('\r', '') for n in pos_list1]
    with codecs.open(path_not, 'r', encoding='UTF-8') as f:
        not_ = f.readlines()
    not_list1 = [ n.replace('\n', '') for n in not_]
    not_list = [n.replace('\r', '') for n in not_list1]
    #Remove cÃ¡c kÃ½ tá»± kÃ©o dÃ i: vd: Ä‘áº¹ppppppp
    text = re.sub(r'([A-Z])\1+', lambda m: m.group(1).upper(), text, flags=re.IGNORECASE)
    # Chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng
    text = text.lower()

    #Chuáº©n hÃ³a tiáº¿ng Viá»‡t, xá»­ lÃ½ emoj, chuáº©n hÃ³a tiáº¿ng Anh, thuáº­t ngá»¯
    replace_list = {
        'Ã²a': 'oÃ ', 'Ã³a': 'oÃ¡', 'á»a': 'oáº£', 'Ãµa': 'oÃ£', 'á»a': 'oáº¡', 'Ã²e': 'oÃ¨', 'Ã³e': 'oÃ©','á»e': 'oáº»',
        'Ãµe': 'oáº½', 'á»e': 'oáº¹', 'Ã¹y': 'uá»³', 'Ãºy': 'uÃ½', 'á»§y': 'uá»·', 'Å©y': 'uá»¹','á»¥y': 'uá»µ','á»§a':'uáº£',
        'aÌ‰': 'áº£', 'Ã´Ì': 'á»‘', 'uÂ´': 'á»‘','Ã´Ìƒ': 'á»—', 'Ã´Ì€': 'á»“', 'Ã´Ì‰': 'á»•', 'Ã¢Ì': 'áº¥', 'Ã¢Ìƒ': 'áº«', 'Ã¢Ì‰': 'áº©',
        'Ã¢Ì€': 'áº§', 'oÌ‰': 'á»', 'ÃªÌ€': 'á»','ÃªÌƒ': 'á»…', 'ÄƒÌ': 'áº¯', 'uÌ‰': 'á»§', 'ÃªÌ': 'áº¿', 'Æ¡Ì‰': 'á»Ÿ', 'iÌ‰': 'á»‰',
        'eÌ‰': 'áº»', 'Ã k': u' Ã  ','aË‹': 'Ã ', 'iË‹': 'Ã¬', 'ÄƒÂ´': 'áº¯','Æ°Ì‰': 'á»­', 'eËœ': 'áº½', 'yËœ': 'á»¹', 'aÂ´': 'Ã¡',
        #Quy cÃ¡c icon vá» 2 loáº¡i emoj: TÃ­ch cá»±c hoáº·c tiÃªu cá»±c
        "ğŸ‘¹": "nagative", "ğŸ‘»": "positive", "ğŸ’ƒ": "positive",'ğŸ¤™': ' positive ', 'ğŸ‘': ' positive ',
        "ğŸ’„": "positive", "ğŸ’": "positive", "ğŸ’©": "positive","ğŸ˜•": "nagative", "ğŸ˜±": "nagative", "ğŸ˜¸": "positive",
        "ğŸ˜¾": "nagative", "ğŸš«": "nagative",  "ğŸ¤¬": "nagative","ğŸ§š": "positive", "ğŸ§¡": "positive",'ğŸ¶':' positive ',
        'ğŸ‘': ' nagative ', 'ğŸ˜£': ' nagative ','âœ¨': ' positive ', 'â£': ' positive ','â˜€': ' positive ',
        'â™¥': ' positive ', 'ğŸ¤©': ' positive ', 'like': ' positive ', 'ğŸ’Œ': ' positive ',
        'ğŸ¤£': ' positive ', 'ğŸ–¤': ' positive ', 'ğŸ¤¤': ' positive ', ':(': ' nagative ', 'ğŸ˜¢': ' nagative ',
        'â¤': ' positive ', 'ğŸ˜': ' positive ', 'ğŸ˜˜': ' positive ', 'ğŸ˜ª': ' nagative ', 'ğŸ˜Š': ' positive ',
        '?': ' ? ', 'ğŸ˜': ' positive ', 'ğŸ’–': ' positive ', 'ğŸ˜Ÿ': ' nagative ', 'ğŸ˜­': ' nagative ',
        'ğŸ’¯': ' positive ', 'ğŸ’—': ' positive ', 'â™¡': ' positive ', 'ğŸ’œ': ' positive ', 'ğŸ¤—': ' positive ',
        '^^': ' positive ', 'ğŸ˜¨': ' nagative ', 'â˜º': ' positive ', 'ğŸ’‹': ' positive ', 'ğŸ‘Œ': ' positive ',
        'ğŸ˜–': ' nagative ', 'ğŸ˜€': ' positive ', ':((': ' nagative ', 'ğŸ˜¡': ' nagative ', 'ğŸ˜ ': ' nagative ',
        'ğŸ˜’': ' nagative ', 'ğŸ™‚': ' positive ', 'ğŸ˜': ' nagative ', 'ğŸ˜': ' positive ', 'ğŸ˜„': ' positive ',
        'ğŸ˜™': ' positive ', 'ğŸ˜¤': ' nagative ', 'ğŸ˜': ' positive ', 'ğŸ˜†': ' positive ', 'ğŸ’š': ' positive ',
        'âœŒ': ' positive ', 'ğŸ’•': ' positive ', 'ğŸ˜': ' nagative ', 'ğŸ˜“': ' nagative ', 'ï¸ğŸ†—ï¸': ' positive ',
        'ğŸ˜‰': ' positive ', 'ğŸ˜‚': ' positive ', ':v': '  positive ', '=))': '  positive ', 'ğŸ˜‹': ' positive ',
        'ğŸ’“': ' positive ', 'ğŸ˜': ' nagative ', ':3': ' positive ', 'ğŸ˜«': ' nagative ', 'ğŸ˜¥': ' nagative ',
        'ğŸ˜ƒ': ' positive ', 'ğŸ˜¬': ' ğŸ˜¬ ', 'ğŸ˜Œ': ' ğŸ˜Œ ', 'ğŸ’›': ' positive ', 'ğŸ¤': ' positive ', 'ğŸˆ': ' positive ',
        'ğŸ˜—': ' positive ', 'ğŸ¤”': ' nagative ', 'ğŸ˜‘': ' nagative ', 'ğŸ”¥': ' nagative ', 'ğŸ™': ' nagative ',
        'ğŸ†—': ' positive ', 'ğŸ˜»': ' positive ', 'ğŸ’™': ' positive ', 'ğŸ’Ÿ': ' positive ',
        'ğŸ˜š': ' positive ', 'âŒ': ' nagative ', 'ğŸ‘': ' positive ', ';)': ' positive ', '<3': ' positive ',
        'ğŸŒ': ' positive ',  'ğŸŒ·': ' positive ', 'ğŸŒ¸': ' positive ', 'ğŸŒº': ' positive ',
        'ğŸŒ¼': ' positive ', 'ğŸ“': ' positive ', 'ğŸ…': ' positive ', 'ğŸ¾': ' positive ', 'ğŸ‘‰': ' positive ',
        'ğŸ’': ' positive ', 'ğŸ’': ' positive ', 'ğŸ’¥': ' positive ', 'ğŸ’ª': ' positive ',
        'ğŸ’°': ' positive ',  'ğŸ˜‡': ' positive ', 'ğŸ˜›': ' positive ', 'ğŸ˜œ': ' positive ',
        'ğŸ™ƒ': ' positive ', 'ğŸ¤‘': ' positive ', 'ğŸ¤ª': ' positive ','â˜¹': ' nagative ',  'ğŸ’€': ' nagative ',
        'ğŸ˜”': ' nagative ', 'ğŸ˜§': ' nagative ', 'ğŸ˜©': ' nagative ', 'ğŸ˜°': ' nagative ', 'ğŸ˜³': ' nagative ',
        'ğŸ˜µ': ' nagative ', 'ğŸ˜¶': ' nagative ', 'ğŸ™': ' nagative ', ':((' : ' nagative ', 
        #Chuáº©n hÃ³a 1 sá»‘ sentiment words/English words
        ':))': '  positive ', ':)': ' positive ', 'Ã´ kÃªi': ' ok ', 'okie': ' ok ', ' o kÃª ': ' ok ',
        'okey': ' ok ', 'Ã´kÃª': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okÃª':' ok ',
        ' tks ': u' cÃ¡m Æ¡n ', 'thks': u' cÃ¡m Æ¡n ', 'thanks': u' cÃ¡m Æ¡n ', 'ths': u' cÃ¡m Æ¡n ', 'thank': u' cÃ¡m Æ¡n ',
        'â­': 'star ', '*': 'star ', 'ğŸŒŸ': 'star ', 'ğŸ‰': u' positive ',
        ' kg ': u' khÃ´ng ',' not ': u' khÃ´ng ', u' kg ': u' khÃ´ng ', '"k ': u' khÃ´ng ',' kh ':u' khÃ´ng ',' kÃ´ ':u' khÃ´ng ',' hok ':u' khÃ´ng ',' kp ': u' khÃ´ng pháº£i ',u' kÃ´ ': u' khÃ´ng ', '"ko ': u' khÃ´ng ', u' ko ': u' khÃ´ng ', u' k ': u' khÃ´ng ', 'khong': u' khÃ´ng ', u' hok ': u' khÃ´ng ',
        'he he': ' positive ','hehe': ' positive ','hihi': ' positive ', 'haha': ' positive ', 'hjhj': ' positive ',
        ' lol ': ' nagative ',' cc ': ' nagative ',' cute ': u' dá»… thÆ°Æ¡ng ',' huhu ': ' nagative ', ' vs ': u' vá»›i ', ' wa ': ' quÃ¡ ', ' wÃ¡ ': u' quÃ¡ ', ' j ': u' gÃ¬ ', 'â€œ': ' ',
        ' sz ': u' cá»¡ ', 'size': u' cá»¡ ', u' Ä‘x ': u' Ä‘Æ°á»£c ', 'dk ': u'Ä‘Æ°á»£c ', 'dc': u' Ä‘Æ°á»£c ', 'Ä‘k ': u'Ä‘Æ°á»£c ',
        ' Ä‘c ': u' Ä‘Æ°á»£c ',' authentic ': u' chuáº©n chÃ­nh hÃ£ng ',u' aut ': u' chuáº©n chÃ­nh hÃ£ng ', u' auth ': u' chuáº©n chÃ­nh hÃ£ng ', ' thick ': u' positive ', ' store ': u' cá»­a hÃ ng ',
        ' shop ': u' cá»­a hÃ ng ', ' sp ': u' sáº£n pháº©m ', ' gud ': u' tá»‘t ',' god ': u' tá»‘t ',' wel done ':' tá»‘t ', ' good ': u' tá»‘t ', ' gÃºt ': u' tá»‘t ',
        ' sáº¥u' : u' xáº¥u ',' gut' : u' tá»‘t ', u' tot ': u' tá»‘t ', u' nice ': u' tá»‘t ', 'perfect': 'ráº¥t tá»‘t', ' bt ': u' bÃ¬nh thÆ°á»ng ',
        ' time ': u' thá»i gian ', ' qÃ¡' : u' quÃ¡ ', u' ship ': u' giao hÃ ng ', u' m ': u' mÃ¬nh ', u' mik ': u' mÃ¬nh ', u'quÃ¡ hÃ i lÃ²g' : u'positive',
        'ÃªÌ‰': 'á»ƒ', 'product': 'sáº£n pháº©m', 'quality': 'cháº¥t lÆ°á»£ng','chat':' cháº¥t ', 'excelent': 'hoÃ n háº£o', 'bad': 'tá»‡','fresh': ' tÆ°Æ¡i ','sad ': 'tá»‡ ',
        ' date ': u' háº¡n sá»­ dá»¥ng ', ' hsd ': u' háº¡n sá»­ dá»¥ng ',' quickly ': u' nhanh ', ' quick ': u' nhanh ','fast': u' nhanh ',' delivery ': u' giao hÃ ng ',u' sÃ­p ': u' giao hÃ ng ',
        ' beautiful ': u' Ä‘áº¹p tuyá»‡t vá»i ', u' tl ': u' tráº£ lá»i ', u' r ': u' rá»“i ', u' shopE ': u' cá»­a hÃ ng ',u' order ': u' Ä‘áº·t hÃ ng ',
        ' cháº¥t lg ': u' cháº¥t lÆ°á»£ng ',u' sd ': u' sá»­ dá»¥ng ',u' dt ': u' Ä‘iá»‡n thoáº¡i ',u' nt ': u' nháº¯n tin ',u' tl ': u' tráº£ lá»i ',u' sÃ i ': u' xÃ i ',u'bjo':u' bao giá» ',
        ' thik ': u' thÃ­ch ',u' sop ': u' cá»­a hÃ ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' ráº¥t ',u' quáº£ ng ':u' quáº£ng  ',
        ' dep ': u' Ä‘áº¹p ',u' xau ': u' xáº¥u ',' delicious ': u' ngon ', u' hÃ g ': u' hÃ ng ', u' qá»§a ': u' quáº£ ',
        ' iu ': u' yÃªu ',' fake ': u' giáº£ máº¡o ', 'trl': 'tráº£ lá»i', '><': u' positive ', u' thÃ­ch Ä‘Ã¡ng ' : ' nagative ',
        ' por ': u' tá»‡ ',' poor ': u' tá»‡ ', ' ib ':u' nháº¯n tin ', ' rep ':u' tráº£ lá»i ',u'fback':' feedback ',' fedback ':' feedback ',
        #dÆ°á»›i 3* quy vá» 1*, trÃªn 3* quy vá» 5*
        '6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ','5sao': ' 5star ',
        'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 1star ','2sao':' 1star ',
        '2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ',}

    for k, v in replace_list.items():
        text = text.replace(k, v)
        
  #    for k, v in replace_list.items():
  #        text = text.replace(k, v)
    
  #    for k in pos_list :
  #      text = text.replace(k, ' positive ')
  #    for k in nag_list :
  #      text = text.replace(k, ' nagative ')
    text = ViTokenizer.tokenize(text)
    
  #    for k, v in replace_list.items():
  #        text = text.replace(k, v)
        
    
    text = text.strip()
    texts = text.split()
    len_text = len(texts)
    for i in range(len_text):
        cp_text = texts[i]
        if cp_text in not_list: 
            numb_word = 2 if len_text - i - 1 >= 4 else len_text - i - 1
           
            for j in range(numb_word):
                if texts[i + j + 1] in pos_list or texts[i+j+1] == 'positive':
                    texts[i] = 'nagative'
                    texts[i + j + 1] = ''

                if texts[i + j + 1] in nag_list or texts[i+j+1] == 'nagative':
                    texts[i] = 'positive'
                    texts[i + j + 1] = ''
  #        else: 
  #            if cp_text in pos_list:
  # #                texts.append('positive')
  #                  texts[i] = 'positive'
  #            elif cp_text in nag_list:
  # #                texts.append('nagative')
  #                  texts[i] = 'nagative'

    text = u' '.join(texts)

    
    text = text.replace(u'"', u' ')
    text = text.replace(u'ï¸', u'')
    text = text.replace('ğŸ»','')
    # text = self.no_marks(text)
    return text
  #load data for word2vec train model 
  def load_data_word2vecTrain(self,path):
    X=[]
    result = []
    sentences= u""
    with open(path, encoding="utf8") as fr: 
      lines = fr.readlines()
      for line in lines: 
        line = re.sub('train_[0-9][0-9][0-9][0-9][0-9][0-9]', '', line)
        line = re.sub('\n','',line)
        sentences += line
        
    X1 = sentences.strip().split('"')
    index = 1 
    while index < len(X1) :
      X1[index] = X1[index].strip()
      if X1[index] != '0' and X1[index] != '1' :
        X1[index] = self.normalize_text(X1[index]) 
        X1[index] = re.sub('\{','',X1[index])
        X1[index] = re.sub('\}','',X1[index])
        X1[index] = re.sub('\=','',X1[index])
        X1[index] = re.sub('\(','',X1[index])
        X1[index] = re.sub('\)','',X1[index])
        X1[index] = re.sub('\,',' ',X1[index])
        if X1[index] != '' and X1[index] != ' ':
          X.append(X1[index])
          index +=1 
          while X1[index] != '0' and X1[index] != '1' :
            index +=1 
      index += 1
      
    for sentences in X: 
      for sentence in sentences.strip().split('.'):
        k = sentence.split()
        if k != []:
          result.append(k)
    return result


  #load data LSTM for train model 
  def load_data_LSTM(self,path, model_path):
    """
    input: 
    path: data path 
    model_path : word2vec model path 
    return : 
    S: 3D-array data convert to vector by word2vec model loaded in model_path
    L : 2D-array label of data 
    """
    X = [] 
    S = [] 
    L = [] 
    Z = []
    K = []
    sentences= ""
    with open(path,encoding="utf8") as fr: 
      lines = fr.readlines()
      for line in lines: 
        line = re.sub('\n','',line)
        line = line.strip()
        sentences += line
    X1 = re.compile('train_[0-9][0-9][0-9][0-9][0-9][0-9]').split(sentences) 
    index = 1 
    while index < len(X1) :
      i = 0
      while X1[index][len(X1[index]) - 1-i] != '1' and X1[index][len(X1[index]) - 1-i]!= '0' :
        i +=1 
      tmp = [0,0]
      tmp[int(X1[index][len(X1[index]) - 1-i],10)] = 1
      L.append(tmp)
      
      X1[index] = X1[index][:len(X1[index]) - 1-i]
      K.append(X1[index])
      X1[index] = re.sub('\.',' ',X1[index])
      X1[index] = self.normalize_text(X1[index]) 
      X1[index] = re.sub('\{','',X1[index])
      X1[index] = re.sub('\}','',X1[index])
      X1[index] = re.sub('\=','',X1[index])
      X1[index] = re.sub('\(','',X1[index])
      X1[index] = re.sub('\)','',X1[index])
      X1[index] = re.sub('\,',' ',X1[index])
      
        
      X.append(X1[index])
      index += 1
  
    if self.modelw2v == None:
      self.modelw2v = Word2Vec.load(model_path)
    model = self.modelw2v
    for line in X :
      tmp = []
      for w in line.split():
        if w != ' ' :
          if w in model.wv :
            tmp.append(model.wv[w])
          else :
            tmp.append(np.zeros(shape=(model.wv[model.wv.index2word[0]].shape[0],), dtype=float))
      Z.append(line)
      if len(tmp) > self.maxlen_sentence :
        tmp = tmp[:self.maxlen_sentence]
      elif len(tmp) < self.maxlen_sentence :
        if len(tmp) == 0:
          tmp= np.zeros(shape=(1,100), dtype=float)
        tmp = np.concatenate((tmp, np.zeros(shape=(self.maxlen_sentence - len(tmp),100), dtype=float)),axis=0)
      S.append(tmp)
       
    return np.array(S),np.array(L), Z, K
  #load test data 
  def load_data_test(self,path_data, model_path):
    X = [] 
    S = [] 
    
    sentences= ""
    with open(path_data) as fr: 
      lines = fr.readlines()
      for line in lines: 
        line = re.sub('\n','',line)
        sentences += line
        
    X1 = re.compile('test_[0-9][0-9][0-9][0-9][0-9][0-9]').split(sentences)    
    index = 1 
    while index < len(X1) :
      X1[index] = re.sub('\"','',X1[index])
      
      X1[index] = re.sub('\.',' ',X1[index])
      X1[index] = self.normalize_text(X1[index]) 
      X1[index] = re.sub('\{','',X1[index])
      X1[index] = re.sub('\}','',X1[index])
      X1[index] = re.sub('\=','',X1[index])
      X1[index] = re.sub('\(','',X1[index])
      X1[index] = re.sub('\)','',X1[index])
      X1[index] = re.sub('\,',' ',X1[index])
        
      X.append(X1[index])
      index += 1
    
    if self.modelw2v == None:
      self.modelw2v = Word2Vec.load(model_path)
    model = self.modelw2v
    for line in X :
      tmp = []
      for w in line.split():
        if w != ' ' :
          if w in model.wv :
            tmp.append(model.wv[w])
          else :
            tmp.append(np.zeros(shape=(model.wv[model.wv.index2word[0]].shape[0],), dtype=float))
      if len(tmp) > 40:
        tmp = tmp[:40]
      elif len(tmp) < 40:
        if len(tmp) == 0:
          tmp= np.zeros(shape=(1,100), dtype=float)
        tmp = np.concatenate((tmp, np.zeros(shape=(40 - len(tmp),100), dtype=float)),axis=0)
      S.append(tmp)
       
    return np.array(S)
  #train model LSTM 
  def train_model_LSTM(self,path_model,X,Y):
    """
    path_model : path for save model 
    X : 3D-array data content 
    Y : 2D-array label 
    """
    model = Sequential()
    inputdim = (X.shape[1], X.shape[2])
    model.add(LSTM(64, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))
    model.add(Dropout(self.dropout))
    model.add(LSTM(32))
    model.add(Dense(self.num_class, activation="softmax"))
  
    model.compile(loss=keras.losses.categorical_crossentropy,
    optimizer=keras.optimizers.Adadelta(),
    metrics=['accuracy'])
    # model.load_weights('./data/LSTM.model73')
    
    model.fit(X, Y, batch_size=self.batch_size, epochs=self.epochs)
    model.save_weights(path_model)

  #predict data 
  def predict(self,path_data_test,model_path_word2vec, model_path_LSTM):
    X,Y,Z,K = self.load_data_LSTM(path_data_test, model_path_word2vec)
    X = X[int(len(X)*0.8):]
    Y = Y[int(len(Y)*0.8):]
    Z = Z[int(len(Z)*0.8):]
    K = K[int(len(K)*0.8):]
    inputdim = (X.shape[1], X.shape[2])
    model = Sequential()
    model.add(LSTM(64, return_sequences=True, input_shape=inputdim))
    model.add(Dropout(self.dropout))
    model.add(LSTM(32))
    model.add(Dense(self.num_class, activation="softmax"))
  
    model.compile(loss=keras.losses.categorical_crossentropy,
                  optimizer=keras.optimizers.Adadelta(),
                  metrics=['accuracy'])
    model.load_weights(model_path_LSTM)
    
    #  X = load_data_test(path_data_test,model_path_word2vec)
    
    yhat = model.predict(np.array(X))
    yhat = np.argmax(yhat, axis=1)
    num = 0 
    submit = []
    
    #  with open('/content/drive/My Drive/colab/sample_submission.csv') as fr:
    #    lines = fr.readlines()
    #    for line in lines:
    #      line = re.sub('test_[0-9][0-9][0-9][0-9][0-9][0-9]','',line)
    #      line = re.sub('\,','',line)
    #      if(int(line) == 0 or int(line) == 1) :
    #        submit.append(int(line))
    #  for i in range(len(yhat)):
    #    if yhat[i] == submit[i]  :
    #      num += 1
  
    for i in range(len(yhat)):
      if Y[i][yhat[i]] == 1 :
        num += 1 
      else :
        print(i)
        print(Z[i])
        print(K[i])
        print(yhat[i])
        print(Y[i])
        
    print(len(yhat))
    print(len(Y))
    print(len(Z))
    print(len(X))
    print("predict : " + str(num))
    print(num/len(yhat))
    print(yhat[:20])
  #predict sentence 
  def convert_sentence_tovec(self,text, model_path_word2vec):
    
    text = re.sub('\"','',text)
    text = re.sub('\.',' ',text)
    text = self.normalize_text(text) 
    text = re.sub('\{','',text)
    text = re.sub('\}','',text)
    text = re.sub('\=','',text)
    text = re.sub('\(','',text)
    text = re.sub('\)','',text)
    text = re.sub('\,',' ',text)

    if self.modelw2v == None:
      self.modelw2v = Word2Vec.load(model_path_word2vec)
    modelw2v = self.modelw2v
    Wvec = []
    tmp = []
    for w in text.split():
      if w != ' ' :
        if w in modelw2v.wv :
          tmp.append(modelw2v.wv[w])
        else :
          tmp.append(np.zeros(shape=(modelw2v.wv[modelw2v.wv.index2word[0]].shape[0],), dtype=float))
    if len(tmp) > 40:
      tmp = tmp[:40]
    elif len(tmp) < 40:
      if len(tmp) == 0:
        tmp= np.zeros(shape=(1,100), dtype=float)
      tmp = np.concatenate((tmp, np.zeros(shape=(40 - len(tmp),100), dtype=float)),axis=0)
    Wvec.append(tmp)
    return np.array(Wvec)


  #predic sentence converted to vector by word2vec and return 
  def predict_sentence(self, text,model_path_word2vec, model_path_LSTM):
    Wvec = self.convert_sentence_tovec(text,model_path_word2vec)
    if self.modelLSTM == None:

      inputdim = (Wvec.shape[1], Wvec.shape[2])
      self.modelLSTM = Sequential()
      self.modelLSTM.add(LSTM(64, return_sequences=True, input_shape=inputdim))
      self.modelLSTM.add(Dropout(self.dropout))
      self.modelLSTM.add(LSTM(32))
      self.modelLSTM.add(Dense(self.num_class, activation="softmax"))
      self.modelLSTM.compile(loss=keras.losses.categorical_crossentropy,
                            optimizer=keras.optimizers.Adadelta(),
                            metrics=['accuracy'])
      self.modelLSTM.load_weights(model_path_LSTM)

    model = self.modelLSTM
    yhat = model.predict(np.array(Wvec))
    yhat = np.argmax(yhat, axis=1)
    return yhat[0] 



  #get randomtext 
  def get_RanText(self,pathdata):
    if self.randText == None:
      sentences= ""
      X  =[]
      L = []
      with open(pathdata,encoding="utf8") as fr: 
        lines = fr.readlines()
        for line in lines: 
          line = re.sub('\n','',line)
          line = line.strip()
          sentences += line
      X1 = re.compile('train_[0-9][0-9][0-9][0-9][0-9][0-9]').split(sentences)
      X1 = X1[int(0.8*len(X1)) :] 
      index = 1 
      while index < len(X1) :
        i = 0
        while X1[index][len(X1[index]) - 1-i] != '1' and X1[index][len(X1[index]) - 1-i]!= '0' :
          i +=1 
        tmp = [0,0]
        tmp[int(X1[index][len(X1[index]) - 1-i],10)] = 1
        L.append(tmp)
        X1[index] = X1[index][:len(X1[index]) - 1-i]
        
        X.append(X1[index])
        index += 1
      self.randText = X
      self.label = L
    index = random.randint(0,len(self.randText))
    return self.randText[index], self.label[index]

if __name__ == "__main__":
  mod = Train()
  text = mod.load_data_word2vecTrain('./data/train.crash')
  model = Word2Vec(text, size=100, window=5, min_count=1, workers=4)
  model.save('./data/word2vec.model2')
  X,Y ,Z,K= mod.load_data_LSTM('./data/train.crash','./data/word2vec.model2')
  X = X[:int(len(X)*0.8)]
  Y = Y[:int(len(Y)*0.8)]
  mod.train_model_LSTM('./data/LSTM.model73',X,Y)
  mod.predict('./data/train.crash','./data/word2vec.model2','./data/LSTM.model73')
  # if mod.predict_sentence("shop phuc vu kem, san pham khong duoc nhu mong doi :( ",'./data/word2vec.model2','./data/LSTM.model73') == 1:
  #   print("tieu cuc")
  # else:
  #   print("tic cuc")
  pass